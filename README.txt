=====================================================================================================
Homework 1:
Random file could be generated by using util\generator.py using syntax:
generator.py	random_input random_update random_lookup

Indexing on primary key of table
python main.py -d database.dat --create -i input.txt
Running with input file composed of ~100,000 pairs of words took around 3s
python main.py -d database.dat --update -i update.txt
Running with update file composed of ~30,000 pairs of words took around 190s
python main.py -d database.dat --lookup -i lookup.txt
Running with lookup file composed of ~30,000 pairs of words (and words) took around 3s

Without indexing on primary key, inserting took only 2s, but updating and looking up took like forever
so I didn't report the completion time.
=====================================================================================================
Homework 2:
Parameters to be parsed:
usage: main.py [-h] [-m {evaluate,train,sample}] -s Source_language
               Source_file_name -t Target_language Target_file_name
               [-i MAX_ITER] [-c CONVERGENCE] [-x SAMPLING_NUMBER] [-n {0,1}]
               [-v {0,1}] [-D Source_dictionary Target_dictionary]
               [-d MODEL_FILE]

IBM Model 1 Machine Translator

optional arguments:
  -h, --help            show this help message and exit
  -m {evaluate,train,sample}, --mode {evaluate,train,sample}
                        Specify either training or evaluating or sampling
                        mode.
  -s Source_language Source_file_name, --source Source_language Source_file_name
                        Specify the source language (the foreign language in
                        IBM model) and source file.
  -t Target_language Target_file_name, --target Target_language Target_file_name
                        Specify the target language (the English language in
                        IBM model) and target file.
  -i MAX_ITER, --max_iter MAX_ITER
                        Specify the maximum iteration.
  -c CONVERGENCE, --convergence CONVERGENCE
                        Specify the convergence difference to stop looping.
                        Default = 0.1
  -x SAMPLING_NUMBER, --sampling_number SAMPLING_NUMBER
                        Specify the number of sentences should be sampled.
  -n {0,1}, --null {0,1}
                        Specify whether we use NULL token or not. Default = 0
                        = False
  -v {0,1}, --verbatim {0,1}
                        Specify whether we should print out some more
                        information. Default = 1 = True
  -D Source_dictionary Target_dictionary, --dictionary Source_dictionary Target_dictionary
                        Specify the dictionary files to be saved for training,
                        or loaded for testing. The file name should be in the
                        order source dictionary file name then target
                        dictionary file name
  -d MODEL_FILE, --model_file MODEL_FILE
                        Specify model file name.

Use of the parameters for the Spanish-English data.

===Sampling into two 100 sentences files
-s es-en\europarl-v7.es-en.es Spanish -t es-en\europarl-v7.es-en.en English -m sample -x 100

The result will be too files with 100 sentences:
es-en\europarl-v7.es-en.es.100.sampling
es-en\europarl-v7.es-en.en.100.sampling

===Training with the sampled sentences:
	- Divergence condition = 1;
	- Number of maximum iteration = 30
	- Using null token
-s es-en\europarl-v7.es-en.es.100.sampling Spanish -t es-en\europarl-v7.es-en.en.100.sampling English 
-D es-en\europarl-v7.es-en.es.100.dict es-en\europarl-v7.es-en.en.100.dict -m train -d model_100.npy 
-i 30 -c 1 -n 1

===Testing with fabricated input
	- Using null token
-s es-en\europarl-v7.es-en.es.100.test Spanish -t es-en\europarl-v7.es-en.en.100.test English 
-D es-en\europarl-v7.es-en.es.100.dict es-en\europarl-v7.es-en.en.100.dict -m evaluate -d model_100.npy 
-n 1

=====================================================================================================